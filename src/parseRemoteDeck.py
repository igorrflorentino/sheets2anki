"""
M√≥dulo de An√°lise de Decks Remotos

Este m√≥dulo implementa a funcionalidade para baixar e processar dados de decks
remotos a partir de planilhas do Google Sheets em formato TSV.

Funcionalidades principais:
- Download de dados TSV de URLs remotas
- Valida√ß√£o de headers e estrutura dos dados
- Processamento de campos e cria√ß√£o de tags hier√°rquicas
- Detec√ß√£o de cards cloze
- Tratamento robusto de erros

Fluxo de processamento:
1. getRemoteDeck() -> Download e coordena√ß√£o geral
2. parse_tsv_data() -> An√°lise inicial dos dados TSV
3. build_remote_deck_from_tsv() -> Constru√ß√£o do deck
4. create_tags_from_fields() -> Cria√ß√£o de sistema hier√°rquico de tags

Classes:
- RemoteDeck: Representa um deck carregado de fonte remota
- RemoteDeckError: Exce√ß√£o customizada para erros de deck remoto

Fun√ß√µes principais:
- getRemoteDeck(): Fun√ß√£o principal para obter e processar deck remoto
- parse_tsv_data(): An√°lise e valida√ß√£o de dados TSV
- build_remote_deck_from_tsv(): Constru√ß√£o do objeto deck a partir dos dados
- create_tags_from_fields(): Cria√ß√£o de tags hier√°rquicas dos campos

Autor: Sheets2Anki Project
"""

# =============================================================================
# IMPORTS
# =============================================================================

import csv
import urllib.request
import urllib.error
import socket
import re  # Para express√µes regulares
from . import column_definitions as cols  # Defini√ß√µes centralizadas de colunas

# =============================================================================
# CUSTOM EXCEPTIONS
# =============================================================================

class RemoteDeckError(Exception):
    """Exce√ß√£o customizada para erros relacionados a decks remotos."""
    pass

# =============================================================================
# CORE CLASSES
# =============================================================================

class RemoteDeck:
    """
    Classe que representa um deck carregado de uma fonte TSV remota.
    
    Esta classe encapsula os dados de um deck obtido de uma planilha
    remota, incluindo quest√µes, m√≠dia e metadados.
    
    Attributes:
        deckName (str): Nome do deck
        questions (list): Lista de quest√µes/cards do deck
        media (list): Lista de arquivos de m√≠dia associados
        ignored_count (int): N√∫mero de cards ignorados devido √† coluna SYNC?
    """
    
    def __init__(self):
        """Inicializa um deck remoto vazio."""
        self.deckName = ""
        self.questions = []  # Mant√©m o atributo 'questions' para compatibilidade
        self.media = []
        self.ignored_count = 0  # Contador de cards ignorados

    def getMedia(self):
        """
        Retorna a lista de arquivos de m√≠dia do deck.
        
        Returns:
            list: Lista de arquivos de m√≠dia
        """
        return self.media

# =============================================================================
# MAIN PROCESSING FUNCTIONS
# =============================================================================

def getRemoteDeck(url):
    """
    Obt√©m e processa um deck remoto a partir de uma URL TSV.
    
    Esta √© a fun√ß√£o principal que coordena todo o processo de:
    1. Valida√ß√£o da URL
    2. Modifica√ß√£o da URL para garantir valores calculados
    3. Download dos dados com fallback para URLs alternativas
    4. Decodifica√ß√£o do conte√∫do
    5. An√°lise e valida√ß√£o dos dados TSV
    6. Constru√ß√£o do objeto RemoteDeck
    
    Args:
        url (str): URL da planilha em formato TSV
        
    Returns:
        RemoteDeck: Objeto contendo o deck processado
        
    Raises:
        RemoteDeckError: Se houver erro no download, decodifica√ß√£o ou processamento
    """
    original_url = url
    
    # 0. Validar URL antes de process√°-la
    validation_result = validate_google_sheets_url(url)
    if not validation_result['valid']:
        issues = '; '.join(validation_result['issues'])
        suggestions = '; '.join(validation_result['suggestions']) if validation_result['suggestions'] else "Verifique se a URL est√° correta"
        raise RemoteDeckError(f"URL inv√°lida: {issues}. Sugest√µes: {suggestions}")
    
    # 1. Garantir que a URL retorne valores calculados, n√£o f√≥rmulas
    try:
        processed_url = ensure_values_only_url(url)
        if processed_url != url:
            # Logging para debug (pode ser removido em produ√ß√£o)
            pass  # print(f"üìù URL modificada para garantir valores calculados: {processed_url}")
    except Exception as e:
        # Se falhar na modifica√ß√£o da URL, continuar com a original
        processed_url = url
    
    # 2. Preparar lista de URLs para tentar (principal + fallbacks)
    urls_to_try = [processed_url]
    fallback_urls = create_fallback_urls(processed_url, validation_result)
    urls_to_try.extend(fallback_urls)
    
    # 3. Tentar download com cada URL
    last_error = None
    successful_url = None
    
    for attempt, try_url in enumerate(urls_to_try, 1):
        try:
            # Headers apropriados
            headers = {
                'User-Agent': 'Mozilla/5.0 (Sheets2Anki) AnkiAddon'
            }
            request = urllib.request.Request(try_url, headers=headers)
            
            response = urllib.request.urlopen(request, timeout=30)
            
            if response.getcode() != 200:
                raise Exception(f"C√≥digo de status inesperado: {response.getcode()}")
            
            successful_url = try_url
            break  # Sucesso! Sair do loop
            
        except socket.timeout:
            last_error = RemoteDeckError(f"Timeout ao acessar URL (30s). Verifique sua conex√£o ou tente novamente.")
            continue
        except urllib.error.HTTPError as e:
            if e.code == 404:
                # Para erro 404, preparar mensagem espec√≠fica
                if attempt == 1:
                    last_error = RemoteDeckError(
                        f"Planilha n√£o encontrada (HTTP 404). Poss√≠veis causas:\n"
                        f"‚Ä¢ Planilha n√£o est√° compartilhada publicamente\n"
                        f"‚Ä¢ URL est√° incorreta ou incompleta\n"
                        f"‚Ä¢ Planilha foi movida ou deletada\n"
                        f"‚Ä¢ GID (ID da aba) est√° incorreto\n\n"
                        f"Tentando URLs alternativas..."
                    )
                else:
                    last_error = RemoteDeckError(
                        f"Planilha n√£o encontrada ap√≥s {attempt} tentativas. Verifique:\n"
                        f"‚Ä¢ Se a planilha est√° compartilhada publicamente\n"
                        f"‚Ä¢ Se a URL est√° correta: {original_url}\n"
                        f"‚Ä¢ Se a aba especificada existe"
                    )
            elif e.code == 403:
                last_error = RemoteDeckError(
                    f"Acesso negado (HTTP 403). A planilha n√£o est√° compartilhada publicamente.\n"
                    f"V√° em Arquivo > Compartilhar > Alterar para 'Qualquer pessoa com o link'"
                )
                break  # Para 403, n√£o tentar outras URLs
            else:
                last_error = RemoteDeckError(f"Erro HTTP {e.code}: {e.reason}")
            continue
        except urllib.error.URLError as e:
            if isinstance(e.reason, socket.timeout):
                last_error = RemoteDeckError(f"Timeout ao acessar URL. Verifique sua conex√£o ou tente novamente.")
            elif isinstance(e.reason, socket.gaierror):
                last_error = RemoteDeckError(f"Erro de DNS. Verifique sua conex√£o com a internet.")
            else:
                last_error = RemoteDeckError(f"Erro de conex√£o: {str(e.reason)}")
            continue
        except Exception as e:
            last_error = RemoteDeckError(f"Erro inesperado de rede ao baixar TSV: {e}")
            continue
    
    # Se nenhuma URL funcionou, lan√ßar o √∫ltimo erro
    if successful_url is None:
        if last_error:
            raise last_error
        else:
            raise RemoteDeckError(f"Falha ao acessar a planilha ap√≥s {len(urls_to_try)} tentativas")
    
    # 4. Decodifica√ß√£o do conte√∫do
    try:
        tsv_data = response.read().decode('utf-8')
    except UnicodeDecodeError as e:
        raise RemoteDeckError(f"Erro ao decodificar conte√∫do TSV: {e}")

    # 5. An√°lise e constru√ß√£o do deck
    try:
        data = parse_tsv_data(tsv_data)
        remoteDeck = build_remote_deck_from_tsv(data)
        # Armazenar URLs para uso posterior
        remoteDeck.url = original_url
        remoteDeck.successful_url = successful_url
        return remoteDeck
    except Exception as e:
        raise RemoteDeckError(f"Erro ao processar deck remoto: {e}")

# =============================================================================
# DATA PARSING AND VALIDATION FUNCTIONS
# =============================================================================

def validate_tsv_headers(headers):
    """
    Valida se o TSV possui os headers obrigat√≥rios.
    
    Verifica se todos os campos obrigat√≥rios definidos em column_definitions
    est√£o presentes no arquivo TSV.
    
    Args:
        headers (list): Lista de headers do arquivo TSV
        
    Returns:
        list: Lista de headers em mai√∫sculas para padroniza√ß√£o
        
    Raises:
        ValueError: Se algum header obrigat√≥rio estiver faltando
    """
    headers_upper = [h.strip().upper() for h in headers]
    missing_headers = [h for h in cols.REQUIRED_COLUMNS if h not in headers_upper]
    
    if missing_headers:
        raise ValueError(f"Colunas obrigat√≥rias faltando: {', '.join(missing_headers)}")
    
    return headers_upper

def parse_tsv_data(tsv_data):
    """
    Analisa e valida dados TSV.
    
    Processa o conte√∫do TSV bruto e valida sua estrutura b√°sica,
    garantindo que possui headers e pelo menos uma linha de dados.
    
    Args:
        tsv_data (str): Conte√∫do TSV bruto como string
        
    Returns:
        list: Lista de listas representando as linhas do TSV
        
    Raises:
        ValueError: Se o arquivo estiver vazio, mal formatado ou sem dados suficientes
    """
    try:
        reader = csv.reader(tsv_data.splitlines(), delimiter='\t')
        data = list(reader)
        
        if not data:
            raise ValueError("Arquivo TSV est√° vazio")
            
        if len(data) < 2:  # Pelo menos headers e uma linha
            raise ValueError("Arquivo TSV deve conter headers e pelo menos um card")
            
        return data
    except csv.Error as e:
        raise ValueError(f"Formato TSV inv√°lido: {str(e)}")

# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================

def clean_formula_errors(cell_value):
    """
    Limpa valores de erro comuns de f√≥rmulas do Google Sheets e f√≥rmulas n√£o calculadas.
    
    O Google Sheets pode exportar valores de erro quando f√≥rmulas n√£o conseguem
    ser calculadas corretamente, resultando em textos como '#NAME?', '#REF!', etc.
    Tamb√©m pode exportar f√≥rmulas como texto quando n√£o consegue calcul√°-las.
    Esta fun√ß√£o detecta e trata esses valores.
    
    Args:
        cell_value (str): Valor da c√©lula a ser limpo
        
    Returns:
        str: Valor limpo, string vazia se for um erro de f√≥rmula ou f√≥rmula n√£o calculada
        
    Examples:
        >>> clean_formula_errors('#NAME?')
        ''
        >>> clean_formula_errors('=SUM(A1:A10)')
        ''
        >>> clean_formula_errors('Conte√∫do normal')
        'Conte√∫do normal'
    """
    if not cell_value or not isinstance(cell_value, str):
        return cell_value
    
    # Lista de valores de erro comuns do Google Sheets/Excel
    formula_errors = [
        '#NAME?',    # Fun√ß√£o ou nome n√£o reconhecido
        '#REF!',     # Refer√™ncia inv√°lida
        '#VALUE!',   # Tipo de valor incorreto
        '#DIV/0!',   # Divis√£o por zero
        '#N/A',      # Valor n√£o dispon√≠vel
        '#NULL!',    # Intersec√ß√£o inv√°lida
        '#NUM!',     # Erro num√©rico
        '#ERROR!',   # Erro gen√©rico
    ]
    
    cell_value_stripped = cell_value.strip()
    
    # Verificar se o valor √© um erro de f√≥rmula
    if cell_value_stripped in formula_errors:
        return ""  # Retornar string vazia para valores de erro
    
    # Verificar se come√ßa com # (poss√≠vel erro n√£o listado)
    if cell_value_stripped.startswith('#') and cell_value_stripped.endswith('!'):
        return ""  # Tratar como erro de f√≥rmula
    
    # Verificar se √© uma f√≥rmula n√£o calculada (novo)
    if detect_formula_content(cell_value_stripped):
        return ""  # Tratar f√≥rmula n√£o calculada como vazio
    
    return cell_value

def clean_formula_errors_with_logging(cell_value, field_name="", row_num=None):
    """
    Limpa valores de erro de f√≥rmulas com logging para debug.
    
    Vers√£o estendida de clean_formula_errors que registra quando
    erros de f√≥rmula ou f√≥rmulas n√£o calculadas s√£o detectados e limpos.
    
    Args:
        cell_value (str): Valor da c√©lula a ser limpo
        field_name (str): Nome do campo (para logging)
        row_num (int): N√∫mero da linha (para logging)
        
    Returns:
        str: Valor limpo, string vazia se for um erro de f√≥rmula ou f√≥rmula n√£o calculada
    """
    if not cell_value or not isinstance(cell_value, str):
        return cell_value
    
    original_value = cell_value
    cleaned_value = clean_formula_errors(cell_value)
    
    # Se o valor foi alterado (erro de f√≥rmula ou f√≥rmula detectada), registrar
    if original_value.strip() != cleaned_value:
        location_info = ""
        if field_name:
            location_info += f"campo '{field_name}'"
        if row_num:
            location_info += f" linha {row_num}" if location_info else f"linha {row_num}"
        
        # Determinar tipo de problema
        problem_type = "erro de f√≥rmula"
        if detect_formula_content(original_value.strip()):
            problem_type = "f√≥rmula n√£o calculada"
        
        # Para ambientes de desenvolvimento, voc√™ pode descomentar esta linha:
        # print(f"‚ö†Ô∏è  {problem_type.title()} detectado e limpo: '{original_value.strip()}' ‚Üí '' ({location_info})")
    
    return cleaned_value

def has_cloze_deletion(text):
    """
    Verifica se o texto cont√©m deletions cloze no formato {{c1::text}}.
    
    Utiliza express√£o regular para detectar padr√µes de cloze deletion
    que s√£o usados para criar cards de preenchimento de lacunas.
    
    Args:
        text (str): Texto a ser verificado
        
    Returns:
        bool: True se cont√©m cloze deletions, False caso contr√°rio
    """
    return bool(re.search(r'\{\{c\d+::.+?\}\}', text))

def clean_tag_text(text):
    """
    Limpa texto para uso em tags removendo caracteres especiais.
    
    Remove caracteres especiais e converte espa√ßos em underscores
    para criar tags compat√≠veis com o Anki.
    
    Args:
        text (str): Texto a ser limpo
        
    Returns:
        str: Texto limpo e formatado para tag, ou string vazia se inv√°lido
    """
    if not text or not text.strip():
        return ""
    return re.sub(r'[^\w\s]', '', text.strip()).replace(' ', '_')

# =============================================================================
# TAG CREATION FUNCTIONS
# =============================================================================

def create_tags_from_fields(fields):
    """
    Cria tags hier√°rquicas a partir de campos espec√≠ficos.
    
    Esta fun√ß√£o processa diferentes campos do card e cria um sistema
    hier√°rquico de tags para melhor organiza√ß√£o no Anki.
    
    Estrutura de tags criadas:
    - topicos::topico1::subtopicos::subtopico1
    - topicos::topico1::conceitos::conceito1
    - bancas::banca1
    - provas::ano1
    - carreiras::carreira1
    - importancia::nivel_importancia
    - variado::tag_adicional1
    
    Args:
        fields (dict): Dicion√°rio contendo os campos do card
        
    Returns:
        list: Lista de tags hier√°rquicas compat√≠veis com o Anki
        
    Examples:
        >>> fields = {'TOPICO': 'Direito Civil', 'SUBTOPICO': 'Contratos', 'CONCEITO': 'Responsabilidade'}
        >>> create_tags_from_fields(fields)
        ['topicos::Direito_Civil::subtopicos::Contratos', 'topicos::Direito_Civil::conceitos::Responsabilidade']
    """
    tags = []
    
    # Processar TOPICO, SUBTOPICO e CONCEITO de forma hier√°rquica
    topico = clean_tag_text(fields.get(cols.TOPICO, ''))
    subtopicos_raw = fields.get(cols.SUBTOPICO, '')
    conceitos_raw = fields.get(cols.CONCEITO, '')
    
    if topico:
        # Processar SUBTOPICO dentro do TOPICO
        if subtopicos_raw:
            # Dividir subt√≥picos por v√≠rgula e processar cada um
            for subtopico in subtopicos_raw.split(','):
                subtopico_clean = clean_tag_text(subtopico)
                if subtopico_clean:
                    tags.append(f"topicos::{topico}::subtopicos::{subtopico_clean}")
        
        # Processar CONCEITO dentro do TOPICO
        if conceitos_raw:
            # Dividir conceitos por v√≠rgula e processar cada um
            for conceito in conceitos_raw.split(','):
                conceito_clean = clean_tag_text(conceito)
                if conceito_clean:
                    tags.append(f"topicos::{topico}::conceitos::{conceito_clean}")
        
        # Se n√£o h√° subt√≥picos nem conceitos, criar tag apenas com t√≥pico principal
        if not subtopicos_raw and not conceitos_raw:
            tags.append(f"topicos::{topico}")
            
    # Processar BANCAS (pode ter m√∫ltiplos valores separados por v√≠rgula)
    bancas = fields.get(cols.BANCAS, '')
    if bancas:
        for banca in bancas.split(','):
            banca_clean = clean_tag_text(banca)
            if banca_clean:
                tags.append(f"bancas::{banca_clean}")
                
    # Processar ANO
    ano = clean_tag_text(fields.get(cols.ANO, ''))
    if ano:
        tags.append(f"provas::{ano}")
        
    # Processar CARREIRA (pode ter m√∫ltiplos valores separados por v√≠rgula)
    carreira = fields.get(cols.CARREIRA, '')
    if carreira:
        for carr in carreira.split(','):
            carr_clean = clean_tag_text(carr)
            if carr_clean:
                tags.append(f"carreiras::{carr_clean}")
        
    # Processar IMPORTANCIA
    importancia = clean_tag_text(fields.get(cols.IMPORTANCIA, ''))
    if importancia:
        tags.append(f"importancia::{importancia}")
        
    # Processar MORE_TAGS (tags adicionais)
    more_tags = fields.get(cols.MORE_TAGS, '')
    if more_tags:
        for tag in more_tags.split(','):
            tag_clean = clean_tag_text(tag)
            if tag_clean:
                tags.append(f"variado::{tag_clean}")
    
    return tags

# =============================================================================
# DECK BUILDING FUNCTIONS
# =============================================================================

def build_remote_deck_from_tsv(data):
    """
    Constr√≥i um objeto RemoteDeck a partir de dados TSV processados.
    
    Esta fun√ß√£o √© respons√°vel pela constru√ß√£o completa do deck, incluindo:
    1. Valida√ß√£o e processamento de headers
    2. Processamento linha por linha dos dados
    3. Valida√ß√£o de campos obrigat√≥rios
    4. Cria√ß√£o de tags hier√°rquicas
    5. Constru√ß√£o das quest√µes finais
    6. Contagem de cards ignorados devido √† coluna SYNC?
    
    Args:
        data (list): Lista de listas contendo os dados TSV (headers + rows)
        
    Returns:
        RemoteDeck: Objeto deck completamente processado e validado
        
    Raises:
        ValueError: Se os headers forem inv√°lidos ou dados estiverem malformados
    """
    # 1. Processar e validar headers
    headers = validate_tsv_headers(data[0])
    
    # 2. Criar mapeamento de header para √≠ndice
    header_indices = {header: idx for idx, header in enumerate(headers)}
    
    # 3. Processar cada linha de dados
    questions = []
    ignored_count = 0
    for row_num, row in enumerate(data[1:], start=2):
        question = _process_tsv_row(row, headers, header_indices, row_num)
        if question:  # Apenas adicionar se a quest√£o for v√°lida
            questions.append(question)
        else:
            # Verificar se foi ignorada devido √† coluna SYNC?
            if _row_ignored_due_to_sync(row, headers, header_indices):
                ignored_count += 1

    # 4. Criar e retornar deck
    remoteDeck = RemoteDeck()
    remoteDeck.deckName = "Deck from TSV"
    remoteDeck.questions = questions
    remoteDeck.ignored_count = ignored_count

    return remoteDeck

def _row_ignored_due_to_sync(row, headers, header_indices):
    """
    Verifica se uma linha foi ignorada especificamente devido √† coluna SYNC?.
    
    Args:
        row (list): Dados da linha
        headers (list): Lista de headers
        header_indices (dict): Mapeamento de header para √≠ndice
        
    Returns:
        bool: True se a linha foi ignorada devido √† coluna SYNC?, False caso contr√°rio
    """
    # Verificar se a linha n√£o est√° vazia
    if not any(cell.strip() for cell in row):
        return False
    
    # Verificar se o comprimento da linha √© v√°lido
    if len(row) != len(headers):
        return False
    
    # Criar dicion√°rio de campos
    fields = _create_fields_dict(row, headers, header_indices)
    
    # Verificar se os campos obrigat√≥rios est√£o presentes
    if not fields[cols.ID] or not fields[cols.PERGUNTA]:
        return False
    
    # Se chegou at√© aqui, a linha s√≥ foi rejeitada se should_sync_question retornou False
    return not cols.should_sync_question(fields)

def _process_tsv_row(row, headers, header_indices, row_num):
    """
    Processa uma √∫nica linha do TSV e cria uma quest√£o.
    
    Args:
        row (list): Dados da linha atual
        headers (list): Lista de headers
        header_indices (dict): Mapeamento de header para √≠ndice
        row_num (int): N√∫mero da linha (para debug)
        
    Returns:
        dict: Quest√£o processada ou None se inv√°lida ou n√£o deve ser sincronizada
    """
    # Pular linhas vazias
    if not any(cell.strip() for cell in row):
        return None

    # Validar comprimento da linha
    if len(row) != len(headers):
        return None

    # Criar dicion√°rio de campos
    fields = _create_fields_dict(row, headers, header_indices, row_num)

    # Validar campos obrigat√≥rios
    if not fields[cols.ID] or not fields[cols.PERGUNTA]:
        return None

    # Verificar se a quest√£o deve ser sincronizada
    if not cols.should_sync_question(fields):
        return None

    # Gerar tags dos campos
    tags = create_tags_from_fields(fields)
    
    # Criar e retornar quest√£o com tags
    return {
        'fields': fields,
        'tags': tags
    }

def _create_fields_dict(row, headers, header_indices, row_num=None):
    """
    Cria dicion√°rio de campos a partir da linha de dados.
    
    Aplica limpeza nos valores das c√©lulas para remover erros de f√≥rmulas
    do Google Sheets (como #NAME?, #REF!, etc.).
    
    Args:
        row (list): Dados da linha
        headers (list): Lista de headers
        header_indices (dict): Mapeamento de header para √≠ndice
        row_num (int, optional): N√∫mero da linha para logging
        
    Returns:
        dict: Dicion√°rio com campos preenchidos e limpos
    """
    fields = {}
    for header in headers:
        idx = header_indices[header]
        if idx < len(row):
            raw_value = row[idx].strip()
            # Limpar erros de f√≥rmula antes de armazenar (com logging)
            cleaned_value = clean_formula_errors_with_logging(raw_value, header, row_num)
            fields[header] = cleaned_value
        else:
            fields[header] = ""
    
    return fields

def ensure_values_only_url(url):
    """
    Garante que a URL do Google Sheets retorne apenas valores calculados, n√£o f√≥rmulas.
    
    O Google Sheets tem diferentes par√¢metros de exporta√ß√£o:
    - format=tsv: Exporta em formato TSV
    - gid=: ID da aba (se espec√≠fica)
    - output=tsv: For√ßa output TSV
    - exportFormat=tsv: Formato de exporta√ß√£o
    
    Para garantir valores calculados ao inv√©s de f√≥rmulas:
    - Usar /export? ao inv√©s de /edit#gid=
    - Adicionar par√¢metros que for√ßam valor calculado
    
    Args:
        url (str): URL original da planilha
        
    Returns:
        str: URL modificada para garantir valores calculados
        
    Examples:
        >>> url = "https://docs.google.com/spreadsheets/d/ABC123/edit#gid=0"
        >>> ensure_values_only_url(url)
        "https://docs.google.com/spreadsheets/d/ABC123/export?format=tsv&gid=0"
    """
    import re
    from urllib.parse import urlparse, parse_qs
    
    # Se n√£o √© URL do Google Sheets, retornar como est√°
    if 'docs.google.com/spreadsheets' not in url:
        return url
    
    # Extrair ID da planilha - considerar tanto URLs normais (/d/ID) quanto publicadas (/d/e/ID)
    # Tentar primeiro o formato publicado /d/e/ID
    spreadsheet_id_match = re.search(r'/spreadsheets/d/e/([a-zA-Z0-9-_]+)', url)
    if spreadsheet_id_match:
        spreadsheet_id = spreadsheet_id_match.group(1)
        # Para URLs publicadas, apenas garantir que est√£o no formato correto
        # N√£o converter para /export pois isso n√£o funciona para URLs publicadas
        return url
    
    # Tentar formato normal /d/ID
    spreadsheet_id_match = re.search(r'/spreadsheets/d/([a-zA-Z0-9-_]+)', url)
    if not spreadsheet_id_match:
        return url
    
    spreadsheet_id = spreadsheet_id_match.group(1)
    
    # Extrair GID se presente
    gid = "0"  # Padr√£o
    
    # Tentar extrair GID do fragmento (#gid=123)
    if '#gid=' in url:
        gid_match = re.search(r'#gid=([^&\s]+)', url)
        if gid_match:
            gid = gid_match.group(1)
    
    # Tentar extrair GID dos par√¢metros (?gid=123)
    elif '?gid=' in url or '&gid=' in url:
        parsed = urlparse(url)
        params = parse_qs(parsed.query)
        if 'gid' in params:
            gid = params['gid'][0]
    
    # Construir URL de export que garante valores calculados
    export_url = f"https://docs.google.com/spreadsheets/d/{spreadsheet_id}/export"
    export_url += f"?format=tsv&exportFormat=tsv&gid={gid}"
    
    return export_url

def detect_formula_content(cell_value):
    """
    Detecta se o conte√∫do da c√©lula ainda cont√©m uma f√≥rmula n√£o calculada.
    
    Al√©m dos erros de f√≥rmula (#NAME?, #REF!, etc.), algumas vezes o Google Sheets
    pode exportar a pr√≥pria f√≥rmula como texto. Esta fun√ß√£o detecta esses casos.
    
    Args:
        cell_value (str): Valor da c√©lula
        
    Returns:
        bool: True se parecer ser uma f√≥rmula, False caso contr√°rio
        
    Examples:
        >>> detect_formula_content('=SUM(A1:A10)')
        True
        >>> detect_formula_content('=VLOOKUP(B2,D:E,2,FALSE)')
        True
        >>> detect_formula_content('Texto normal')
        False
        >>> detect_formula_content('=5+3')
        True
        >>> detect_formula_content('= n√£o √© f√≥rmula')
        False
        >>> detect_formula_content('=')
        False
    """
    if not cell_value or not isinstance(cell_value, str):
        return False
    
    cell_value = cell_value.strip()
    
    # Verificar se √© apenas o sinal de igual
    if cell_value == '=':
        return False
    
    # Verificar se come√ßa com = mas n√£o √© uma f√≥rmula v√°lida
    if cell_value.startswith('='):
        # Deve ter pelo menos um caractere ap√≥s o =
        if len(cell_value) <= 1:
            return False
        
        # Verificar padr√µes de f√≥rmulas v√°lidas
        formula_content = cell_value[1:]  # Removes o =
        
        # Se cont√©m apenas espa√ßos ap√≥s =, n√£o √© f√≥rmula
        if not formula_content.strip():
            return False
        
        # Se cont√©m palavras comuns que indicam que n√£o √© f√≥rmula
        non_formula_indicators = [
            ' n√£o ', ' nao ', ' is ', ' are ', ' was ', ' were ',
            ' the ', ' and ', ' or ', ' but ', ' if ', ' when ',
            ' para ', ' com ', ' sem ', ' por ', ' em ', ' de ',
        ]
        
        for indicator in non_formula_indicators:
            if indicator in cell_value.lower():
                return False
        
        # Verificar padr√µes comuns de f√≥rmulas v√°lidas
        valid_formula_patterns = [
            r'^=\w+\(',  # =FUNCAO(
            r'^=\d+[\+\-\*\/]',  # =5+, =10*, etc.
            r'^=[A-Z]+\d+',  # =A1, =B2, etc.
            r'^=.*\([^\)]*\).*$',  # Qualquer fun√ß√£o com par√™nteses
            r'^=\d+(\.\d+)?([\+\-\*\/]\d+(\.\d+)?)+$',  # Opera√ß√µes matem√°ticas: =5+3, =10*2.5
        ]
        
        for pattern in valid_formula_patterns:
            if re.search(pattern, cell_value):
                return True
        
        return False
    
    return False

def validate_google_sheets_url(url):
    """
    Valida se a URL √© uma URL v√°lida do Google Sheets.
    
    Verifica estrutura, ID da planilha e se a URL est√° bem formatada.
    
    Args:
        url (str): URL a ser validada
        
    Returns:
        dict: Resultado da valida√ß√£o com detalhes
        
    Examples:
        >>> validate_google_sheets_url("https://docs.google.com/spreadsheets/d/1ABC123/edit#gid=0")
        {'valid': True, 'spreadsheet_id': '1ABC123', 'gid': '0', 'issues': []}
    """
    import re
    from urllib.parse import urlparse
    
    result = {
        'valid': False,
        'spreadsheet_id': None,
        'gid': None,
        'issues': [],
        'suggestions': []
    }
    
    if not url or not isinstance(url, str):
        result['issues'].append("URL est√° vazia ou n√£o √© uma string")
        return result
    
    url = url.strip()
    
    # Verificar se √© URL do Google Sheets
    if 'docs.google.com/spreadsheets' not in url:
        result['issues'].append("URL n√£o √© do Google Sheets")
        result['suggestions'].append("URL deve conter 'docs.google.com/spreadsheets'")
        return result
    
    # Verificar estrutura b√°sica da URL
    try:
        parsed = urlparse(url)
        if not parsed.scheme or not parsed.netloc:
            result['issues'].append("URL mal formatada (sem scheme ou netloc)")
            return result
    except Exception as e:
        result['issues'].append(f"Erro ao analisar URL: {e}")
        return result
    
    # Extrair ID da planilha - tentar primeiro padr√£o de URL publicada
    spreadsheet_id_match = re.search(r'/spreadsheets/d/e/([^/]+)', url)
    if not spreadsheet_id_match:
        # Tentar padr√£o normal (/d/ID)
        spreadsheet_id_match = re.search(r'/spreadsheets/d/([a-zA-Z0-9-_]+)', url)
        if not spreadsheet_id_match:
            result['issues'].append("ID da planilha n√£o encontrado na URL")
            result['suggestions'].append("URL deve ter formato: /spreadsheets/d/[ID]/ ou /spreadsheets/d/e/[ID]/")
            return result
    
    result['spreadsheet_id'] = spreadsheet_id_match.group(1)
    
    # Extrair GID se presente
    gid = "0"  # Padr√£o
    
    # Tentar extrair GID do fragmento (#gid=123)
    if '#gid=' in url:
        gid_match = re.search(r'#gid=([^&\s]+)', url)
        if gid_match:
            gid = gid_match.group(1)
    
    # Tentar extrair GID dos par√¢metros (?gid=123)
    elif '?gid=' in url or '&gid=' in url:
        from urllib.parse import parse_qs
        parsed = urlparse(url)
        params = parse_qs(parsed.query)
        if 'gid' in params:
            gid = params['gid'][0]
    
    result['gid'] = gid
    
    # Verificar se ID da planilha √© muito curto (erro cr√≠tico)
    if len(result['spreadsheet_id']) < 10:
        result['issues'].append("ID da planilha muito curto - provavelmente inv√°lido")
        result['suggestions'].append("Verifique se a URL est√° completa e correta")
    # Warning para IDs curtos mas possivelmente v√°lidos
    elif len(result['spreadsheet_id']) < 20:
        result['suggestions'].append("ID da planilha parece curto - IDs do Google geralmente t√™m 44+ caracteres")
    
    # Se chegou at√© aqui sem problemas cr√≠ticos, √© v√°lida
    if not result['issues']:
        result['valid'] = True
    
    return result

def create_fallback_urls(original_url, validation_result):
    """
    Cria URLs alternativas para tentar em caso de erro 404.
    
    Args:
        original_url (str): URL original que falhou
        validation_result (dict): Resultado da valida√ß√£o da URL
        
    Returns:
        list: Lista de URLs alternativas para tentar
    """
    fallback_urls = []
    
    if not validation_result.get('valid') or not validation_result.get('spreadsheet_id'):
        return fallback_urls
    
    spreadsheet_id = validation_result['spreadsheet_id']
    gid = validation_result.get('gid', '0')
    
    # Detectar se √© URL publicada (/d/e/) ou normal (/d/)
    is_published = '/d/e/' in original_url
    
    if is_published:
        # Para URLs publicadas, usar formato /d/e/ID/pub
        
        # URL 1: Pub TSV com GID 0
        fallback_urls.append(
            f"https://docs.google.com/spreadsheets/d/e/{spreadsheet_id}/pub?output=tsv&gid=0"
        )
        
        # URL 2: Pub TSV com GID espec√≠fico (se diferente de 0)
        if gid != '0':
            fallback_urls.append(
                f"https://docs.google.com/spreadsheets/d/e/{spreadsheet_id}/pub?output=tsv&gid={gid}"
            )
        
        # URL 3: Pub CSV como fallback
        fallback_urls.append(
            f"https://docs.google.com/spreadsheets/d/e/{spreadsheet_id}/pub?output=csv&gid=0"
        )
        
        # URL 4: Tentar formato tradicional (sem /e/)
        fallback_urls.append(
            f"https://docs.google.com/spreadsheets/d/{spreadsheet_id}/export?format=tsv&gid=0"
        )
    else:
        # Para URLs normais, usar formato /d/ID/export
        
        # URL 1: Export b√°sico com GID 0 (aba padr√£o)
        fallback_urls.append(
            f"https://docs.google.com/spreadsheets/d/{spreadsheet_id}/export?format=tsv&exportFormat=tsv&gid=0"
        )
        
        # URL 2: Export com GID detectado (se diferente de 0)
        if gid != '0':
            fallback_urls.append(
                f"https://docs.google.com/spreadsheets/d/{spreadsheet_id}/export?format=tsv&exportFormat=tsv&gid={gid}"
            )
        
        # URL 3: Export sem GID especificado
        fallback_urls.append(
            f"https://docs.google.com/spreadsheets/d/{spreadsheet_id}/export?format=tsv&exportFormat=tsv"
        )
        
        # URL 4: Export com output=tsv
        fallback_urls.append(
            f"https://docs.google.com/spreadsheets/d/{spreadsheet_id}/export?output=tsv&gid=0"
        )
    
    # Remover duplicatas mantendo ordem
    seen = set()
    unique_fallbacks = []
    for url in fallback_urls:
        if url not in seen and url != original_url:
            seen.add(url)
            unique_fallbacks.append(url)
    
    return unique_fallbacks
