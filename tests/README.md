# ğŸ§ª Testes do Sheets2Anki

Este diretÃ³rio contÃ©m a suÃ­te de testes completa para o add-on Sheets2Anki, desenvolvida com pytest.

## ğŸ“‹ Estrutura dos Testes

### ğŸ“ Arquivos de Teste

| Arquivo | DescriÃ§Ã£o | Cobertura |
|---------|-----------|-----------|
| `conftest.py` | ConfiguraÃ§Ãµes e fixtures comuns | Fixtures reutilizÃ¡veis |
| `test_data_processor.py` | Processamento de dados TSV | Parsing, validaÃ§Ã£o, detecÃ§Ã£o cloze |
| `test_config_manager.py` | Gerenciamento de configuraÃ§Ãµes | CRUD configuraÃ§Ãµes, persistÃªncia |
| `test_utils.py` | FunÃ§Ãµes utilitÃ¡rias | URLs, hashes, validaÃ§Ãµes |
| `test_student_manager.py` | GestÃ£o de alunos | Alunos globais, filtragem |
| `test_integration.py` | Testes de integraÃ§Ã£o | Fluxos completos |
| `test_new_tags.py` | Sistema de tags (legado) | Tags hierÃ¡rquicas |
| `run_tests.py` | Script executor | AutomaÃ§Ã£o de testes |

### ğŸ·ï¸ Categorias de Testes

#### **ğŸ”¬ Testes UnitÃ¡rios** (`@pytest.mark.unit`)
- Testam funÃ§Ãµes isoladas
- ExecuÃ§Ã£o rÃ¡pida
- Mocks para dependÃªncias externas
- Cobertura de casos extremos

#### **ğŸ”— Testes de IntegraÃ§Ã£o** (`@pytest.mark.integration`) 
- Testam fluxos completos
- InteraÃ§Ã£o entre mÃ³dulos
- CenÃ¡rios reais de uso
- ValidaÃ§Ã£o end-to-end

#### **ğŸŒ Testes Lentos** (`@pytest.mark.slow`)
- Testes de performance
- Datasets grandes
- OperaÃ§Ãµes demoradas
- Pulados em execuÃ§Ã£o rÃ¡pida

## ğŸš€ Como Executar os Testes

### PrÃ©-requisitos

```bash
# Instalar dependÃªncias de teste
pip install pytest pytest-cov pytest-mock
```

### ExecuÃ§Ã£o BÃ¡sica

```bash
# Todos os testes
python tests/run_tests.py

# Ou diretamente com pytest
python -m pytest tests/
```

### OpÃ§Ãµes AvanÃ§adas

```bash
# ğŸ”¬ Apenas testes unitÃ¡rios
python tests/run_tests.py --unit

# ğŸ”— Apenas testes de integraÃ§Ã£o  
python tests/run_tests.py --integration

# ğŸ“Š Com cobertura de cÃ³digo
python tests/run_tests.py --coverage

# ğŸƒ ExecuÃ§Ã£o rÃ¡pida (pula testes lentos)
python tests/run_tests.py --fast

# ğŸ” SaÃ­da detalhada
python tests/run_tests.py --verbose

# ğŸ“ Arquivo especÃ­fico
python tests/run_tests.py --file data_processor

# ğŸ¯ FunÃ§Ã£o especÃ­fica
python tests/run_tests.py --function test_parse_students

# â„¹ï¸ InformaÃ§Ãµes sobre testes disponÃ­veis
python tests/run_tests.py --info
```

### CombinaÃ§Ãµes Ãšteis

```bash
# Desenvolvimento: testes unitÃ¡rios com saÃ­da detalhada
python tests/run_tests.py --unit --verbose

# CI/CD: todos os testes com cobertura
python tests/run_tests.py --coverage

# Debug: arquivo especÃ­fico com verbose
python tests/run_tests.py --file utils --verbose

# Performance: apenas testes lentos
python -m pytest tests/ -m slow -v
```

## ğŸ—ï¸ Fixtures DisponÃ­veis

### ğŸ“Š Dados de Teste

- `sample_tsv_data`: Dados TSV estruturados para testes
- `sample_tsv_content`: ConteÃºdo TSV em formato string  
- `sample_students`: Lista de alunos para testes
- `sample_url`: URL de exemplo do Google Sheets
- `sample_config`: ConfiguraÃ§Ã£o padrÃ£o para testes

### ğŸ­ Mocks do Anki

- `mock_mw`: Mock do main window do Anki
- `mock_note`: Mock de uma nota do Anki
- `mock_note_type`: Mock de um note type
- `setup_test_environment`: ConfiguraÃ§Ã£o automÃ¡tica de mocks

### ğŸ“ Arquivos TemporÃ¡rios

- `temp_config_file`: Arquivo de configuraÃ§Ã£o temporÃ¡rio
- `clean_sys_path`: Limpeza do sys.path apÃ³s testes

## ğŸ“ Escrevendo Novos Testes

### Estrutura BÃ¡sica

```python
import pytest

@pytest.mark.unit
class TestMyModule:
    """Testes para o mÃ³dulo MyModule."""
    
    def test_basic_functionality(self):
        """Teste bÃ¡sico de funcionalidade."""
        # Arrange
        input_data = "test_input"
        expected = "expected_output"
        
        # Act  
        result = my_function(input_data)
        
        # Assert
        assert result == expected
    
    def test_edge_cases(self):
        """Teste de casos extremos."""
        assert my_function("") == ""
        assert my_function(None) is None
        
    def test_error_handling(self):
        """Teste de tratamento de erros."""
        with pytest.raises(ValueError):
            my_function("invalid_input")
```

### Usando Fixtures

```python
def test_with_fixtures(self, sample_tsv_data, mock_mw):
    """Teste usando fixtures."""
    # Usar dados de amostra
    assert len(sample_tsv_data) > 0
    
    # Usar mock do Anki
    mock_mw.col.findNotes.return_value = [123, 456]
    result = my_anki_function(mock_mw)
    assert len(result) == 2
```

### Testes de IntegraÃ§Ã£o

```python
@pytest.mark.integration
def test_complete_workflow(self, tmp_path):
    """Teste de fluxo completo."""
    # Criar ambiente temporÃ¡rio
    config_file = tmp_path / "test_config.json"
    
    # Executar fluxo completo
    result = complete_workflow(str(config_file))
    
    # Verificar resultado
    assert result['success'] == True
```

## ğŸ“Š Cobertura de CÃ³digo

### Gerar RelatÃ³rio

```bash
# RelatÃ³rio HTML (recomendado)
python tests/run_tests.py --coverage

# Abrir no navegador
open htmlcov/index.html
```

### MÃ©tricas Alvo

- **Cobertura Total**: > 80%
- **MÃ³dulos CrÃ­ticos**: > 90%
- **FunÃ§Ãµes PÃºblicas**: 100%

### Visualizar Cobertura

O relatÃ³rio HTML mostra:
- âœ… Linhas cobertas (verde)
- âŒ Linhas nÃ£o cobertas (vermelho)  
- âš ï¸ Branches parciais (amarelo)
- ğŸ“Š Percentual por arquivo

## ğŸ› Debugging de Testes

### ExecuÃ§Ã£o com Debug

```bash
# Parar no primeiro erro
python -m pytest tests/ -x

# Mostrar variÃ¡veis locais em falhas
python -m pytest tests/ -l

# Modo debug interativo
python -m pytest tests/ --pdb

# Capturar prints
python -m pytest tests/ -s
```

### Logs de Debug

```python
import logging
logging.basicConfig(level=logging.DEBUG)

def test_with_logging():
    logger = logging.getLogger(__name__)
    logger.debug("Debug info")
    
    # Teste continua...
```

## ğŸ”§ ConfiguraÃ§Ã£o Personalizada

### pytest.ini (jÃ¡ configurado)

```ini
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
markers =
    unit: mark a test as a unit test
    integration: mark a test as an integration test
    slow: mark a test as slow (skipped in quick mode)
addopts = -v
```

### Personalizar Markers

```bash
# Executar apenas testes marcados
python -m pytest tests/ -m "unit and not slow"

# Executar testes com tag personalizada
python -m pytest tests/ -m "requires_anki"
```

## ğŸ¯ Melhores PrÃ¡ticas

### âœ… Do's

- **Use fixtures** para dados comuns
- **Teste casos extremos** (valores nulos, vazios, invÃ¡lidos)
- **Nomes descritivos** para funÃ§Ãµes de teste
- **DocumentaÃ§Ã£o** clara do que estÃ¡ sendo testado
- **Arrange-Act-Assert** pattern
- **Mocks** para dependÃªncias externas

### âŒ Don'ts

- **NÃ£o teste implementaÃ§Ã£o**, teste comportamento
- **NÃ£o use dados hardcoded** desnecessariamente
- **NÃ£o ignore falhas** intermitentes
- **NÃ£o teste cÃ³digo de terceiros**
- **NÃ£o faÃ§a testes muito complexos**

### ğŸ—ï¸ Estrutura Recomendada

```python
class TestMyFeature:
    """Testes agrupados por feature."""
    
    def test_happy_path(self):
        """CenÃ¡rio principal de sucesso."""
        pass
    
    def test_edge_cases(self):
        """Casos extremos."""
        pass
    
    def test_error_handling(self):
        """Tratamento de erros."""
        pass
```

## ğŸ“ˆ MÃ©tricas e RelatÃ³rios

### Executar AnÃ¡lise Completa

```bash
# Cobertura + relatÃ³rios detalhados
python tests/run_tests.py --coverage --verbose

# Ver estatÃ­sticas
python tests/run_tests.py --info
```

### CI/CD Integration

```bash
# Para pipelines automatizados
python -m pytest tests/ \
  --cov=src \
  --cov-report=xml \
  --cov-report=term \
  --junitxml=test-results.xml
```

## ğŸ†˜ Troubleshooting

### Problemas Comuns

#### Import Errors
```bash
# Verificar PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
python -m pytest tests/
```

#### Mock Issues
```python
# Verificar se mocks estÃ£o sendo aplicados corretamente
def test_debug_mock(self, mock_mw):
    print(f"Mock type: {type(mock_mw)}")
    print(f"Mock methods: {dir(mock_mw)}")
```

#### Fixture Conflicts
```python
# Usar fixtures especÃ­ficas quando necessÃ¡rio
def test_specific_fixture(self, sample_tsv_data):
    # Em vez de fixture genÃ©rica, criar dados especÃ­ficos
    specific_data = [{'ID': 'TEST001', 'PERGUNTA': 'Test?'}]
```

### Logs de Debug

```bash
# Ver logs detalhados dos testes
python -m pytest tests/ --log-cli-level=DEBUG
```

## ğŸš€ PrÃ³ximos Passos

### Melhorias Planejadas

- [ ] Testes de performance automatizados
- [ ] Cobertura de mutation testing
- [ ] Testes de carga para datasets grandes
- [ ] IntegraÃ§Ã£o com GitHub Actions
- [ ] Testes de compatibilidade multi-versÃ£o

### Como Contribuir

1. **Identifique gaps** na cobertura
2. **Escreva testes** seguindo os padrÃµes
3. **Execute a suÃ­te** completa antes do commit
4. **Documente** casos complexos
5. **Mantenha fixtures** atualizadas

---

Para mais informaÃ§Ãµes sobre pytest: https://docs.pytest.org/
